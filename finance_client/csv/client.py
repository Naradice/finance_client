import datetime
import difflib
import math
import os
import random
from abc import ABCMeta, abstractmethod

import numpy
import pandas as pd

import finance_client.frames as Frame
from finance_client.client_base import Client


class CSVClientBase(Client, metaclass=ABCMeta):
    kinds = "csv"
    available_slip_type = ["random", "none", "percent", "pct"]

    # functions for read csv
    def _update_columns(self, columns: list):
        temp_columns = {}
        for column in columns:
            column_ = str(column).lower()
            if column_ == "open":
                temp_columns["Open"] = column
            elif column_ == "high":
                temp_columns["High"] = column
            elif column_ == "low":
                temp_columns["Low"] = column
            elif "close" in column_:
                temp_columns["Close"] = column
            elif "time" in column_:  # assume time, timestamp or datetime
                temp_columns["Time"] = column
        self.ohlc_columns.update(temp_columns)

    def _initialize_file_name_func(self, file_names: list):
        def has_strings(file_name: str, strs_list):
            if len(strs_list) > 0:
                for strs in strs_list:
                    if file_name.find(strs) == -1:
                        return False
                return True
            return False

        indices = random.sample(range(0, len(file_names)), k=len(file_names))
        base_file_name = file_names[indices[0]]

        same_strs = []
        for index in indices[1:]:
            file_name = os.path.abspath(file_names[index])
            if has_strings(file_name, same_strs):
                break
            differ = difflib.Differ()
            ans = differ.compare(base_file_name, file_name)
            same_strs = []
            same_str = ""

            for diff in ans:
                d = list(diff)
                if d[0] == "-" or d[0] == "+":
                    if same_str != "":
                        same_strs.append(same_str)
                        same_str = ""
                else:
                    same_str += d[2]

            if same_str != "":
                same_strs.append(same_str)
        self._get_symbol_from_filename = lambda file_name: f"symbol_{self.files.index(os.path.abspath(file_name))}"
        if len(same_strs) > 0:
            if len(same_strs) > 1:
                if len(same_strs) > 2:
                    self.logger.warning("filenames have a few same words. symbol name may be created unexpectedly.")
                prefix = same_strs[0]
                suffix = same_strs[-1]
                self._get_symbol_from_filename = (
                    lambda file_name: os.path.abspath(file_name).replace(prefix, "").replace(suffix, "")
                )
                self.logger.info(
                    f"symbol name for column name is automatically generated by removing prefix {prefix} and suffix {suffix}"
                )
                if self.file_name_generator is None:
                    self.file_name_generator = lambda symbol: f"{prefix}{symbol}{suffix}"
                else:
                    self.logger.info("file_name_generator is not initialized automatically as it is specified manually.")
            else:
                suffix = same_strs[-1]
                self._get_symbol_from_filename = lambda file_name: os.path.abspath(file_name).replace(suffix, "")
                if self.file_name_generator is None:
                    # filename would have .csv, so assume it as suffix
                    self.file_name_generator = lambda symbol: f"{symbol}{suffix}"
                else:
                    self.logger.info("file_name_generator is not initialized automatically as it is specified manually.")

    def _initialize_date_index(self, data: pd.DataFrame, ascending: bool):
        data = data.sort_index(ascending=ascending)

        if ascending:
            delta = data.index[1:] - data.index[:-1]
        else:
            delta = (data.index[:-1] - data.index[1:]).to_series().mode()
        df = delta.to_series()
        delta = df.mode().values[0]
        delta = delta / numpy.timedelta64(1, "s")
        delta = delta / 60

        frame = delta

        if self.frame is None:
            self.frame = frame
        else:
            if self.frame != frame:
                self.logger.warning("different frame data is provided. it break some features. use index frame instead")
                self.frame = frame

        if self.frame < Frame.D1:
            if data.index.tzinfo is None:
                data.index = pd.to_datetime(data.index, utc=True)
        return data

    def _proceed_step_until_date(self, data, start_date):
        is_date_found = True
        if data is not None and len(data) > 0:
            if start_date is not None and type(start_date) is datetime.datetime:
                # start_date = start_date.astimezone(datetime.timezone.utc)
                start_date = start_date.replace(tzinfo=datetime.timezone.utc)
                # TODO: update tz handling.
                remaining_length = len(data.index[data.index >= start_date])
                if remaining_length > 0:
                    start_index = len(data.index) - remaining_length
                    # date is retrievd by [:step_index], so we need to plus 1
                    self._step_index = start_index + 1
                else:
                    self._step_index = len(data.index)
                    self.logger.warning(f"start date {start_date} doesn't exit in the index")
        return is_date_found

    def _create_csv_kwargs(self, columns, date_column, skiprows, is_multi_mode=False):
        usecols = columns
        kwargs = {}
        if date_column is not None:
            # To load date column, don't specify columns if date is not specified at first
            if len(usecols) > 0:
                usecols = set(usecols)
                usecols.add(date_column)
                kwargs["usecols"] = usecols
            kwargs["parse_dates"] = [date_column]
            kwargs["index_col"] = date_column

        # skiplow is applied after concat in case of multi symbols
        if is_multi_mode is False and skiprows is not None:
            # assume index 0 is column
            kwargs["skiprows"] = range(1, skiprows + 1)
        return kwargs

    def _create_dfs_by_files(self, files, symbols, kwargs):
        __symbols = []
        DFS = {}
        handled_files = set(self.files)
        for index in range(0, len(files)):
            file = files[index]
            file = os.path.abspath(file)
            if len(symbols) > index:
                symbol = symbols[index]
            else:
                symbol = self._get_symbol_from_filename(file)
            try:
                df = pd.read_csv(file, header=0, **kwargs)
                handled_files.add(file)
                __symbols.append(symbol)
                DFS[symbol] = df
            except PermissionError as e:
                self.logger.error(f"file, {file}, is handled by other process {e}")
                raise e
            except Exception as e:
                self.logger.error(f"error occured on read csv {e}")
                raise e
        self.files = list(handled_files)

        return DFS, __symbols

    def _make_timecolumn_to_index(self, data, symbols, time_column, columns, is_multi_mode):
        dfs = []
        DFS = {}
        __columns = list(set(columns) - set(time_column))
        # if is_multi_mode:
        dfs = [data[_symbol] for _symbol in symbols]
        # else:
        #     dfs = [data]
        for index in range(0, len(symbols)):
            df = dfs[index].dropna()
            symbol = symbols[index]
            df.set_index(time_column, inplace=True)
            if type(df.index) != pd.DatetimeIndex:
                df.index = pd.to_datetime(df.index, utc=True)
            df = df.sort_index(ascending=True)
            if __columns is not None and len(__columns) > 0:
                df = df[__columns]
            DFS[symbol] = df.copy()
        return DFS

    def _create_datetime_index(self, start_date, frame, data_length):
        if start_date is not None and frame is not None and data_length is not None:
            delta = datetime.timedelta(minutes=frame)
            return pd.Index([start_date + delta * i for i in range(0, data_length)])
        else:
            self.logger.warning("any of parameters are None on create_datetime_index")
            return pd.Index([i for i in range(0, data_length)])

    # functions for get_ohlc
    def _get_target_symbols(self, symbols):
        """separate symbols to available and missing"""
        target_symbols = []
        missing_symbols = []
        missing_files = []
        if len(symbols) == 0:
            target_symbols = self.symbols
        else:
            # check any of provided symbols exists in data
            target_symbols = list(set(self.symbols) & set(symbols))
            if len(target_symbols) == 0:
                # assume file_path is provided instead of symbols
                for file in symbols:
                    _file_path = os.path.abspath(file)
                    if os.path.exists(_file_path):
                        file = _file_path
                        if self._get_symbol_from_filename is None:
                            self._initialize_file_name_func(symbols)
                        __symbol = self._get_symbol_from_filename(file)
                        if __symbol in self.symbols:
                            target_symbols.append(__symbol)
                        else:
                            # filename generate may be different from symbol generator, so store file as is
                            missing_files.append(file)
                    else:
                        # check if prvided symbol string is part of existing filename
                        is_handled_symbol = False
                        for handled_file in self.files:
                            if file in handled_file:
                                # assume __symbol represents symbol name of handled_file
                                __symbol = self._get_symbol_from_filename(handled_file)
                                target_symbols.append(__symbol)
                                is_handled_symbol = True
                                break
                        if is_handled_symbol is False:
                            # it may be not a file name, so store it as is
                            missing_symbols.append(file)

            else:
                # some of provided symbols match with handled symbols, so assume all others are symbol names (not filename)
                missing_symbols = list(set(symbols) - set(self.symbols))
            if len(missing_symbols) > 0:
                if self.file_name_generator is None:
                    self.logger.error("can't create file name from symbol as file name generator is not defined.")
                    self.logger.warning(f"{missing_files} are ignored")
                else:
                    for symbol in missing_symbols:
                        file = self.file_name_generator(symbol)
                        missing_files.append(os.path.abspath(file))
        return target_symbols, missing_files

    def __generate_symbol(self, file_name: str, sps: list):
        symbol = file_name
        for sep in sps:
            symbol = symbol.replace(sep, "")
        return symbol

    def __init__(
        self,
        files: list = [],
        columns=[],
        date_column=None,
        file_name_generator=None,
        symbols=[],
        frame: int = None,
        out_frame: int = None,
        observation_length=None,
        idc_process=None,
        pre_process=None,
        economic_keys=None,
        start_index=0,
        start_date=None,
        keep_observation_length=True,
        start_random_index=False,
        auto_step_index=True,
        skiprows=None,
        auto_reset_index=False,
        slip_type="random",
        budget=1000000,
        do_render=False,
        seed=1017,
        provider="csv",
        logger=None,
    ):
        """CSV Client Base
        Need to change codes to use settings file
        """
        super().__init__(
            budget=budget,
            do_render=do_render,
            symbols=symbols,
            out_ohlc_columns=columns,
            idc_process=idc_process,
            pre_process=pre_process,
            economic_keys=economic_keys,
            frame=frame,
            observation_length=observation_length,
            provider=provider,
            logger_name=__name__,
            logger=logger,
        )
        random.seed(seed)
        self.data = None
        self.files = []
        self.ohlc_columns = {}
        self.auto_step_index = auto_step_index
        self.keep_observation_length = keep_observation_length
        slip_type = slip_type.lower()
        if slip_type in self.available_slip_type:
            if slip_type == "percent" or slip_type == "pct":
                slip_rate = 0.1
                self._get_current_bid = lambda open_value, low_value: open_value - (open_value - low_value) * slip_rate
                self._get_current_ask = lambda open_value, high_value: open_value + (high_value - open_value) * slip_rate
            elif slip_type == "random":
                self._get_current_bid = lambda open_value, low_value: random.uniform(low_value, open_value)
                self._get_current_ask = lambda open_value, high_value: random.uniform(open_value, high_value)
            elif slip_type == "none" or slip_rate is None:
                self._get_current_bid = lambda open_value, low_value: open_value
                self._get_current_ask = lambda open_value, low_value: open_value
        else:
            self.logger.warn(f"{slip_type} is not in availble values: {self.available_slip_type}")
            self.logger.info("use random as slip_type")
            self._get_current_bid = lambda open_value, low_value: random.uniform(low_value, open_value)
            self._get_current_ask = lambda open_value, high_value: random.uniform(open_value, high_value)
        # store args so that we can reproduce CSV client by loading args file
        self._args = {
            "out_frame": out_frame,
            "columns": columns,
            "date_column": date_column,
            "start_index": start_index,
            "start_date": start_date,
            "start_random_index": start_random_index,
            "auto_step_index": auto_step_index,
            "auto_reset_index": auto_reset_index,
            "skiprows": skiprows,
            "slip_type": slip_type,
            "seed": seed,
        }

        self.file_name_generator = file_name_generator
        if file_name_generator is not None:
            sep = "?_?"
            dummpy_path = file_name_generator(sep)
            fixes = dummpy_path.split(sep)
            self._get_symbol_from_filename = lambda file_path: self.__generate_symbol(file_path, fixes)
            if len(symbols) > 0:
                files = list(files)
                for symbol in symbols:
                    file_name = file_name_generator(symbol)
                    files.append(file_name)

        self.base_point = 0.01
        self.frame = frame
        self.symbols = []

        self._auto_reset = auto_reset_index
        _index_update_required = False
        if start_index:
            if start_index >= 0:
                self._step_index = start_index
            else:
                _index_update_required = True
        else:
            self._step_index = 1

        if len(files) > 0:
            if type(files) == str:
                self.files = [os.path.abspath(files)]
            else:
                if type(files) != list:
                    try:
                        files = list(files)
                    except Exception as e:
                        raise Exception(f"files is specified, but can't be casted to list: {e}")
                # store files to be able to reproduce the client by params
                self.files = files
                files = [os.path.abspath(file) for file in self.files]
            self._initialize_file_name_func(files)
            self.data, __symbols = self._read_csv(self.files, symbols, columns, date_column, skiprows, start_date, frame)
            self.symbols = list(__symbols)
            if _index_update_required:
                self._step_index = len(self) + start_index  # assume negative value is specified
            elif start_random_index:
                self._step_index = random.randint(1, len(self))
        if self._step_index > len(self):
            self.logger.warning(f"step index {self._step_index} is greater than data length {len(self)}")

    def get_current_index(self):
        return self._step_index

    def get_params(self) -> dict:
        adds = self.get_additional_params()
        args = self._args.copy()
        args.update(adds)

        return self._args

    def __len__(self):
        if self.data is not None:
            return len(self.data)
        return 0

    @abstractmethod
    def _read_csv(self, files, columns, date_column, skiprows, start_date, frame):
        symbols = []
        return pd.DataFrame(), symbols

    @abstractmethod
    def get_additional_params(self):
        return {}

    # overwrite if needed
    def _update_rates(self, symbols) -> bool:
        """you can define a function to increment length of self.data on parent class

        Returns:
            bool: succeeded to update. Length should be increased 1 at least
        """
        return False

    @abstractmethod
    def _get_ohlc_from_client(
        self,
        length: int = None,
        symbols: list = [],
        frame: int = None,
        columns: list = None,
        index=None,
        grouped_by_symbol: bool = False,
    ):
        pass

    @abstractmethod
    def get_future_rates(self, length=1, back_length=0, symbols: list = []):
        pass

    @abstractmethod
    def get_next_tick(self):
        pass

    @abstractmethod
    def get_current_ask(self, symbols=None):
        pass

    @abstractmethod
    def get_current_bid(self, symbols=None):
        pass

    @abstractmethod
    def reset(self, mode: str = None, retry=0):
        pass

    def __getitem__(self, batch_idx):
        if type(batch_idx) is tuple:
            idx = batch_idx[0]
            columns = batch_idx[1]
            if columns is None:
                df = self.data
            else:
                self.data.columns = self.data.columns.swaplevel(0, 1)
                df = self.data[columns]
                self.data.columns = self.data.columns.swaplevel(0, 1)
        else:
            df = self.data
            idx = batch_idx

        chunk_data = numpy.array([])
        item = numpy.array([])
        chunk_size = 0

        for index in self.indices[idx]:
            item = df.iloc[index - self.observation_length : index].values
            chunk_size += 1
            chunk_data = numpy.append(chunk_data, item)
        chunk_data = chunk_data.reshape(chunk_size, *item.shape)
        return chunk_data


class CSVClient(CSVClientBase):
    def __init__(
        self,
        files: list = [],
        columns=[],
        date_column=None,
        file_name_generator=None,
        symbols=[],
        frame: int = None,
        out_frame: int = None,
        observation_length=None,
        idc_process=None,
        pre_process=None,
        economic_keys=None,
        start_index=0,
        start_date=None,
        start_random_index=False,
        auto_step_index=True,
        keep_observation_length=True,
        skiprows=None,
        auto_reset_index=False,
        slip_type="random",
        budget=1000000,
        do_render=False,
        seed=1017,
        provider="csv",
        logger=None,
    ):
        """CSV Client for time series data like bitcoin, stock, finance

        Args:
            files (list<str>, optional): You can directly specify the file names. Defaults to [].
            columns (list, optional): column names to read from CSV files. Defaults to [].
            date_column (str, optional): If specified, try to parse time columns. Otherwise search time column. Defaults to None
            file_name_generator (function, optional): function to create file name from symbol. Ex) lambda symbol: f'D:\\warehouse\\stock_D1_{symbol}.csv'
            symbols (list, optional): symbols name of data. Used to specify columns names.
            frame (int, optional): input frame. Specify time series span by minutes. Defaults None and determined by data.
            out_frame (int, optional): output frame. Ex) Convert 5MIN data to 30MIN by out_frame=30. Defaults to None.
            observation_length (int, optional): specify data length for __getitem__.
            idc_process (list<Process>): list of technical indicater processes
            pre_process (list<PreProcess>): list of pre-process
            economic_keys (list<str>): list of key of technical indicaters
            start_index (int, optional): specify minimum index. If not specified, start from 0. Defauls to None.
            start_date (datetime, optional): specify start date. start_date overwrite the start_index. If not specified, start from index=0. Defaults to None.
            keep_observation_length(bool, optional): If true, observation_length is kept when index reach to the end. Otherwise observation_length become less than it. Defaults to True.
            start_random_index (bool, optional): After init or reset_index, random index is used as initial index. Defaults to False.
            auto_step_index (bool, optional): If true, get_rate function returns data with advancing the index. Otherwise data index is advanced only when get_next_tick is called
            skiprows (int, optional): specify number to skip row of csv. For multi symbols, row is skipped for each files. Defaults None, not skipped.
            auto_reset_index ( bool, optional): refreh the index when index reach the end. Defaults False
            slip_type (str, optional): Specify how ask and bid slipped. random: random value from Close[i] to High[i] and Low[i]. prcent or pct: slip_rate=0.1 is applied. none: no slip.
            do_render (bool, optional): If true, plot OHLC and supported indicaters.
            seed (int, optional): specify random seed. Defaults to 1017
        """
        super().__init__(
            files,
            columns,
            date_column,
            file_name_generator,
            symbols,
            frame,
            out_frame,
            observation_length,
            idc_process,
            pre_process,
            economic_keys,
            start_index,
            start_date,
            keep_observation_length,
            start_random_index,
            auto_step_index,
            skiprows,
            auto_reset_index,
            slip_type,
            budget,
            do_render,
            seed,
            provider,
            logger,
        )
        if out_frame is not None:
            if self.frame < out_frame:
                # Rolled result has NaN regardless market is open or not.
                self.data = self.roll_ohlc_data(self.data, out_frame, grouped_by_symbol=True)
                self.frame = out_frame
        if self.data is not None and len(self.data) > 0:
            self.data = self.run_processes(self.data, self.symbols, self.idc_process, self.pre_process, True)

    def _read_csv(self, files, symbols=[], columns=[], date_col=None, skiprows=None, start_date=None, frame=None):
        DFS = {}
        __symbols = []
        is_multi_mode = False
        if len(files) > 1:
            is_multi_mode = True

        # create kwargs from provided args
        kwargs = self._create_csv_kwargs(columns, date_col, skiprows, is_multi_mode)
        is_date_index = False
        if "parse_dates" in kwargs:
            is_date_index = True

        # read csvs by pandas feature
        DFS, __symbols = self._create_dfs_by_files(files, symbols, kwargs)
        data = pd.concat(DFS.values(), axis=1, keys=DFS.keys())

        if skiprows is not None and is_multi_mode:
            if skiprows > 0:
                data = data.iloc[skiprows:].copy()

        # update ohlc column info. assume all have same columns. If different column csv has read, overwrite old column info
        __columns = data[__symbols[0]].columns
        self._update_columns(__columns)

        # convert TIME column values to datetime index if date column name is not defined, but there is time column
        if is_date_index is False and "Time" in self.ohlc_columns:
            # make Time column to index
            time_column = self.ohlc_columns["Time"]
            if time_column in __columns:
                DFS = self._make_timecolumn_to_index(data, __symbols, time_column, columns, is_multi_mode)
                data = pd.concat(DFS.values(), axis=1, keys=DFS.keys())
                is_date_index = True
        if is_date_index is False:
            if start_date is not None and frame is not None:
                data.index = self._create_datetime_index(start_date, frame, len(data))
                is_date_index = True
            else:
                raise Exception("Couldn't daterming date column")
        if len(data) > 0:
            data = self._initialize_date_index(data, True)
            self._proceed_step_until_date(data, start_date)
        # read csv is called with different columns when get_ohlc is called with different symbols, so marge managing symbols
        return data, __symbols

    def get_additional_params(self):
        args = {"file": self.files}
        return args

    def __read_missing_symbol_data(self, symbols):
        target_symbols, missing_files = self._get_target_symbols(symbols)
        if len(missing_files) > 0:
            date_column = None
            if "Time" in self.ohlc_columns:
                date_column = self.ohlc_columns["Time"]
            columns = self._args["columns"]
            data, __symbols = self._read_csv(missing_files, [], columns, date_column)
            self.files.extend(list(__symbols))
            if self.data is not None:
                start_date = self.data.index[0]
                data = data[data.index >= start_date]
            return __symbols, data
        return target_symbols, pd.DataFrame()

    def __get_rates(self, index, length, symbols, frame, columns):
        if length is None:
            rates = self.data
            if frame is not None and self.frame < frame:
                rates = self.roll_ohlc_data(rates, frame, grouped_by_symbol=True)
            if len(symbols) > 0:
                rates = rates[symbols]
            if columns is None:
                return rates.iloc[:index]
            else:
                return rates[columns].iloc[:index]
        elif length >= 1:
            rates = None
            out_length = length
            if frame is not None and self.frame < frame:
                length = math.ceil(frame / self.frame) * length
            try:
                # return data which have length length
                if len(symbols) > 0:
                    rates = self.data[symbols]
                else:
                    rates = self.data
                rates = rates[self.get_ohlc_columns(out_type="list", ignore="Time")]
                rates = rates.iloc[index - length : index]
            except Exception as e:
                self.logger.error(f"can't find data fom {index - length} to {index}: {e}")

            if frame is not None and self.frame < frame:
                rates = self.roll_ohlc_data(rates, frame, grouped_by_symbol=True)
            rates = rates.iloc[:out_length]
            if columns is None:
                return rates
            else:
                return rates[columns]
        else:
            raise Exception("interval should be greater than 0.")

    def _get_ohlc_from_client(
        self, length: int = None, symbols: list = [], frame: int = None, columns=None, index=None, grouped_by_symbol: bool = False
    ):
        missing_data = pd.DataFrame()
        target_symbols = []
        try:
            target_symbols, missing_data = self.__read_missing_symbol_data(symbols)
        except Exception:
            self.logger.exception("Filed to read csv on get_ohlc_data of CSV client")

        if len(missing_data) > 0:
            try:
                self.data = pd.concat([self.data, missing_data], axis=1)
            except Exception:
                self.logger.exception("Filed to concat existing data with additional data of specified symbols")

        target_columns = None
        if len(target_symbols) == 0:
            self.logger.warning(f"Specified symbols can't be handled as csv file or its symbol name: {symbols}")
            return pd.DataFrame()
        elif len(target_symbols) == 1:
            target_symbols = target_symbols[0]
            if columns is not None:
                target_columns = columns
        else:
            if columns is not None:
                target_columns = pd.MultiIndex.from_product([target_symbols, columns])

        if index is None:
            index = self._step_index
            if index > len(self):
                if self._update_rates(symbols) is False:
                    if self._auto_reset:
                        # todo initialize based on parameters
                        self._step_index = random.randint(0, len(self.data))
                    else:
                        if self.keep_observation_length:
                            self.logger.warning(
                                f"current step {self._step_index} over the data length {len(self)}. Fix step index to last index"
                            )
                            self._step_index = self._step_index - 1
                index = self._step_index

        if length is not None:
            if index < length - 1:
                self.logger.warning(
                    f"index {index} is less than length {length}. return length from index 0. Please assgin start_index."
                )
                index = length
        else:
            self._update_rates(symbols)
        rates = self.__get_rates(index, length, target_symbols, frame, target_columns)
        if grouped_by_symbol is False and type(rates.columns) is pd.MultiIndex:
            rates.columns = rates.columns.swaplevel(0, 1)
            rates.sort_index(level=0, axis=1, inplace=True)
        if self.auto_step_index:
            self._step_index += 1
        return rates

    def get_future_rates(self, length=1, back_length=0, symbols: list = []):
        if length > 1:
            rates = None
            if self._step_index >= length - 1:
                try:
                    # return data which have length length
                    rates = self.data.iloc[self._step_index - back_length : self._step_index + length + 1].copy()
                    return rates
                except Exception as e:
                    self.logger.error(e)
            else:
                if self._auto_reset:
                    self._step_index = random.randint(0, len(self.data))
                    # raise index change event
                    return self.get_future_rates(length)
                else:
                    self.logger.warning(f"not more data on index {self._step_index}")
                return pd.DataFrame()
        else:
            raise Exception(f"length should be greater than 0. {length} is provided.")

    def get_current_ask(self, symbols=[]):
        last_10_ticks = self.data.iloc[: self._step_index].fillna(method="ffill")
        tick = last_10_ticks.iloc[-1]
        open_column = self.ohlc_columns["Open"]
        high_column = self.ohlc_columns["High"]
        if type(tick.index) is pd.MultiIndex:
            if type(symbols) is str:
                if symbols in self.symbols:
                    open_value = tick[symbols][open_column]
                    high_value = tick[symbols][open_column]
                else:
                    return None
            elif type(symbols) is list:
                if len(symbols) > 0:
                    target_symbols = set(self.symbols) & set(symbols)
                    target_symbols = list(target_symbols)
                else:
                    target_symbols = self.symbols

                if len(target_symbols) == 1:
                    open_value = tick[target_symbols[0]][open_column]
                    high_value = tick[target_symbols[0]][open_column]
                else:
                    open_value = tick[[(__symbol, open_column) for __symbol in target_symbols]]
                    open_value.index = target_symbols
                    high_value = tick[[(__symbol, high_column) for __symbol in target_symbols]]
                    high_value.index = target_symbols
            else:
                err_msg = f"Unknown type is specified as symbols: {type(symbols)}"
                self.logger.error(err_msg)
                raise Exception(err_msg)
        else:
            open_value = tick[open_column]
            high_value = tick[open_column]
        return self._get_current_ask(open_value, high_value)

    def get_current_bid(self, symbols=[]):
        last_10_ticks = self.data.iloc[: self._step_index].fillna(method="ffill")
        tick = last_10_ticks.iloc[-1]
        open_column = self.ohlc_columns["Open"]
        low_column = self.ohlc_columns["Low"]

        if type(tick.index) is pd.MultiIndex:
            if type(symbols) is str:
                if symbols in self.symbols:
                    open_value = tick[symbols][open_column]
                    low_value = tick[symbols][open_column]
                else:
                    return None
            elif type(symbols) is list:
                if len(symbols) > 0:
                    target_symbols = set(self.symbols) & set(symbols)
                    target_symbols = list(target_symbols)
                else:
                    target_symbols = self.symbols

                if len(target_symbols) == 1:
                    open_value = tick[target_symbols[0]][open_column]
                    low_value = tick[target_symbols[0]][open_column]
                else:
                    open_value = tick[[(__symbol, open_column) for __symbol in target_symbols]]
                    open_value.index = target_symbols
                    low_value = tick[[(__symbol, low_column) for __symbol in target_symbols]]
                    low_value.index = target_symbols
            else:
                err_msg = f"Unknown type is specified as symbols: {type(symbols)}"
                self.logger.error(err_msg)
                raise Exception(err_msg)
        else:
            open_value = tick[open_column]
            low_value = tick[open_column]

        return self._get_current_bid(open_value, low_value)

    def reset(self, mode: str = None, retry=0) -> bool:
        self._step_index = random.randint(0, len(self.data))
        # start time mode: hour day, week, etc
        if mode is not None:
            if retry <= 10:
                if mode == "day":
                    date_column = self.ohlc_columns["Time"]
                    current_date = self.data[date_column].iloc[self._step_index]
                    day_minutes = 60 * 24
                    total_minutes = current_date.minute + current_date.hour * 60
                    add_minutes = day_minutes - total_minutes
                    add_index = add_minutes / self.frame
                    if self._step_index + add_index >= len(self.data):
                        self.reset(mode, retry + 1)
                    else:
                        candidate_index = self._step_index + add_index
                        current_date = self.data[date_column].iloc[candidate_index]
                        additional_index = (current_date.hour * 60 + current_date.minute) / self.frame
                        if additional_index != 0:
                            candidate_index = candidate_index - additional_index
                            current_date = self.data[date_column].iloc[candidate_index]
                            total_minutes = current_date.minute + current_date.hour * 60
                            # otherwise, this day may have 1day data
                            if total_minutes != 0:
                                self.reset(mode, retry + 1)
            else:
                self.logger.warning("Data client reset with {mode} mode retried over 10. index was not correctly reset.")
                return False
        # raise index change event
        return True

    def get_next_tick(self):
        if self._step_index < len(self.data) - 2:
            self._step_index += 1
            tick = self.data.iloc[self._step_index]
            # raise index change event
            return tick, False
        else:
            if self._auto_reset:
                self._step_index = random.randint(0, len(self.data))
                # raise index change event
                tick = self.data.iloc[self._step_index]
                return tick, True
            else:
                self.logger.warning(f"not more data on index {self._step_index}")
            return pd.DataFrame(), True

    @property
    def max(self):
        self.data.max()

    @property
    def min(self):
        self.data.max()


class CSVChunkClient(CSVClientBase):
    def __init__(
        self,
        chunksize: int,
        files: list = None,
        columns=[],
        date_column=None,
        file_name_generator=None,
        symbols=[],
        frame: int = None,
        out_frame: int = None,
        observation_length=None,
        start_index=0,
        start_date=None,
        start_random_index=False,
        auto_step_index=True,
        skiprows=None,
        auto_reset_index=False,
        slip_type="random",
        budget=1000000,
        do_render=False,
        seed=1017,
        provider="csv",
        logger=None,
    ):
        """Low memory CSV Client

        Args:
            chunksize (int): To load huge file partially, you can specify chunk size.
            files (list<str>, optional): You can directly specify the file names. Defaults to None.
            columns (list, optional): column names to read from CSV files. Defaults to [].
            date_column (str, optional): If specified, try to parse time columns. Otherwise search time column. Defaults to None
            file_name_generator (function, optional): function to create file name from symbol. Ex) lambda symbol: f'D:\\warehouse\\stock_D1_{symbol}.csv'
            symbols (list, optional): symbols (list, optional): symbols name of data. Used to specify columns names.
            frame (int, optional): input frame. Specify time series span by minutes. Defaults None and determined by data.
            out_frame (int, optional): output frame. Ex) Convert 5MIN data to 30MIN by out_frame=30. Defaults to None.
            observation_length (int, optional): specify data length for __getitem__.
            start_index (int, optional): specify minimum index. If not specified, start from 0. Defauls to None.
            start_date (datetime, optional): specify start date. start_date overwrite the start_index. If not specified, start from index=0. Defaults to None.
            start_random_index (bool, optional): After init or reset_index, random index is used as initial index. Defaults to False.
            auto_step_index (bool, optional): If true, get_rate function returns data with advancing the index. Otherwise data index is advanced only when get_next_tick is called
            skiprows (int, optional): specify number to skip row of csv. For multi symbols, row is skipped for each files. Defaults None, not skipped.
            auto_reset_index ( bool, optional): refreh the index when index reach the end. Defaults False
            slip_type (str, optional): Specify how ask and bid slipped. random: random value from Close[i] to High[i] and Low[i]. prcent or pct: slip_rate=0.1 is applied. none: no slip.
            do_render (bool, optional): If true, plot OHLC and supported indicaters.
            seed (int, optional): specify random seed. Defaults to 1017
        """
        if chunksize < 1:
            raise ValueError("chunksize should be greater than 0.")
        self.TIMES = {}
        self.TRS = {}
        self.chunksize = chunksize
        super().__init__(
            files,
            columns,
            date_column,
            file_name_generator,
            symbols,
            frame,
            out_frame,
            observation_length,
            start_index,
            start_date,
            start_random_index,
            auto_step_index,
            skiprows,
            auto_reset_index,
            slip_type,
            budget,
            do_render,
            seed,
            provider,
            logger,
        )
        data = pd.concat(self.data.values(), axis=1, keys=self.data.keys())
        # to avoid
        is_date_found = self._proceed_step_until_date(data, start_date)
        while is_date_found is False:
            dfs = [self.__read_chunk_data(symbol) for symbol in self.symbols]
            new_data = pd.concat(dfs, axis=1, keys=self.symbols)
            data = self._initialize_date_index(new_data, True)
            is_date_found = self._proceed_step_until_date(data, start_date)
        TIMES = {}
        for symbol in self.symbols:
            df = data[symbol].dropna()
            last_time = df.index[-1]
            TIMES[symbol] = (df.index[0], last_time)
        self.TIMES.update(TIMES)

    def _read_csv(self, files, symbols=[], columns=[], date_col=None, skiprows=None, start_date=None, frame=None):
        is_multi_mode = False
        if len(files) > 1:
            is_multi_mode = True

        # create kwargs from provided args
        kwargs = self._create_csv_kwargs(columns, date_col, skiprows, is_multi_mode)
        kwargs["chunksize"] = self.chunksize
        is_date_index = False
        if "parse_dates" in kwargs:
            is_date_index = True

        # read csvs by pandas feature. When chunksize is specified, TextReader is stored instead of DataFrame
        TRS, __symbols = self._create_dfs_by_files(files, symbols, kwargs)
        # when _read_csv is called twice or more times for same file, ignore it. This case should not be caused.
        duplicated_key_set = set(TRS.keys()) & set(self.TRS.keys())
        if len(duplicated_key_set) > 0:
            self.logger.warning("same file is initialized with chunkmode. TextReader is ignored.")
            for file in duplicated_key_set:
                TRS.pop(file)
        self.TRS.update(TRS)

        DFS = {}
        for key, tr in TRS.items():
            df = tr.get_chunk()
            DFS[key] = df
        data = pd.concat(DFS.values(), axis=1, keys=__symbols)
        if skiprows is not None and is_multi_mode:
            if skiprows > 0:
                data = data.iloc[skiprows:].copy()

        # update ohlc column info. If different column csv has read, overwrite old column info
        __columns = data[__symbols[0]].columns
        self._update_columns(__columns)

        # convert TIME column values to datetime index if date column name is not defined, but there is time column
        if is_date_index is False and "Time" in self.ohlc_columns:
            # make Time column to index
            time_column = self.ohlc_columns["Time"]
            if time_column in __columns:
                DFS = self._make_timecolumn_to_index(data, __symbols, time_column, columns, is_multi_mode)
                data = pd.concat(DFS.values(), axis=1, keys=DFS.keys())
                is_date_index = True
        if is_date_index is False:
            if start_date is not None and frame is not None:
                data.index = self._create_datetime_index(start_date, frame, len(data))
                is_date_index = True
            else:
                raise Exception("Couldn't daterming date column")
            raise Exception("Couldn't daterming date column")
        data = self._initialize_date_index(data, True)
        DFS = {}
        for _symbol in __symbols:
            # store each df. It may reduce the memory usage for NaM. (Is it handled in pandas?)
            DFS[_symbol] = data[_symbol].dropna()
        # read csv is called with different columns when get_ohlc is called with different symbols, so marge managing symbols
        return DFS, __symbols

    def __read_chunk_data(self, symbol):
        if symbol in self.TRS:
            tr = self.TRS[symbol]
            try:
                df = tr.get_chunk()
                return df
            except StopIteration:
                self.TRS.pop(symbol)
            except Exception as e:
                self.logger.error(f"failed read {symbol} file by {e}")
        else:
            self.logger.debug(f"{symbol} is not in TRS")
        return pd.DataFrame()

    def __update_chunkdata_with_time(self, symbols: list = [], interval: int = None):
        min_last_time = datetime.datetime.now()
        min_last_time = min_last_time.astimezone(tz=datetime.timezone.utc)

        for symbol in symbols:
            if symbol not in self.TIMES:
                # if file_name_generator is defined, initialize it
                # else case, check symbol is a part of any column name
                # and else, show warning
                print(f"{symbol} is not initialized")
                pass
            first_time, last_time = self.TIMES[symbol]
            if last_time.tzinfo is None:
                last_time = last_time.tz_localize(tz=datetime.timezone.utc)
            if min_last_time > last_time:
                min_last_time = last_time
                min_time_symbol = symbol

        if len(self.data[min_time_symbol]) - self._step_index < interval:
            DFS = {}
            # update data of min_date symbol.
            temp_df = self.data[min_time_symbol].dropna()
            short_length = interval - len(temp_df) + self._step_index
            short_chunk_count = math.ceil(short_length / self.chunksize)
            tr = self.TRS[min_time_symbol]
            temp_dfs = [temp_df]
            ohlc_columns = self.get_ohlc_columns()
            time_convert_required = False
            if "Time" in ohlc_columns:
                time_column = ohlc_columns["Time"]
                time_convert_required = True

            for count in range(0, short_chunk_count):
                # may happen error
                temp_df = tr.get_chunk()
                if time_convert_required:
                    temp_df.set_index(time_column, inplace=True)
                    if type(temp_df.index) != pd.DatetimeIndex:
                        temp_df.index = pd.to_datetime(temp_df.index, utc=True)
                    temp_df = temp_df.sort_index(ascending=True)
                temp_dfs.append(temp_df)
            temp_df = pd.concat(temp_dfs, axis=0)
            required_last_time = temp_df.index[self._step_index + interval - 1]
            temp_df = temp_df.dropna()

            if self.frame < Frame.D1:
                temp_df.index = pd.to_datetime(temp_df.index, utc=True)
            DFS[min_time_symbol] = temp_df
            self.TIMES[min_time_symbol] = (temp_df.index[0], temp_df.index[-1])
            min_last_time = temp_df.index[-1]

            # update remaining symbols and confirm if symbol of min_date is changed by the updates.
            remaining_symbols = set(self.TRS.keys()) - {min_time_symbol}
            for symbol in remaining_symbols:
                date_set = self.TIMES[symbol]
                symbol_last_date = date_set[1]
                tr = self.TRS[symbol]
                temp_dfs = [self.data[symbol].dropna()]
                while symbol_last_date < required_last_time:
                    temp_df = tr.get_chunk()
                    if time_convert_required:
                        temp_df.set_index(time_column, inplace=True)
                        if type(temp_df.index) != pd.DatetimeIndex:
                            temp_df.index = pd.to_datetime(temp_df.index, utc=True)
                        temp_df = temp_df.sort_index(ascending=True)
                    temp_dfs.append(temp_df)
                    symbol_last_date = temp_df.index[-1]
                if len(temp_df) > 1:
                    temp_df = pd.concat(temp_dfs, axis=0)
                    temp_df = temp_df.dropna()
                    if self.frame < Frame.D1:
                        if type(temp_df.index) != pd.DatetimeIndex:
                            temp_df.index = pd.to_datetime(temp_df.index, utc=True)
                    DFS[symbol] = temp_df
                    self.TIMES[symbol] = (temp_df.index[0], temp_df.index[-1])
                    if min_last_time > temp_df.index[-1]:
                        min_last_time = temp_df.index[-1]
                        min_time_symbol = symbol
                else:
                    DFS[symbol] = self.data[min_time_symbol].dropna()
            self.data.update(DFS)
            temp_df = pd.concat(DFS.values(), axis=1, keys=DFS.keys())
            self._proceed_step_until_date(temp_df, self._args["start_date"])

    def __get_rates_by_chunk(self, interval: int = None, symbols: list = [], frame: int = None):
        target_symbols = list(set(self.symbols) & set(symbols))
        if interval is None:
            # todo: update missing columns
            if self.auto_step_index:
                self._step_index += 1
            if len(target_symbols) > 0:
                rates = pd.concat([self.data[symbol] for symbol in target_symbols], axis=1, keys=target_symbols)
        else:
            self.__update_chunkdata_with_time(target_symbols, interval)
            if len(target_symbols) > 0:
                symbols_dfs = pd.concat([self.data[symbol] for symbol in target_symbols], axis=1, keys=target_symbols)
            else:
                symbols_dfs = pd.concat(self.data.values(), axis=1, keys=self.data.keys())

            if self._step_index >= interval - 1:
                rates = symbols_dfs.iloc[self._step_index - interval : self._step_index]
            else:
                rates = symbols_dfs[:interval]
                self.logger.warning(
                    f"current step index {self._step_index} is less than length {interval}. return length from index 0. Please assgin start_index."
                )

            if self.auto_step_index:
                self._step_index += 1
        return rates

    def _get_ohlc_from_client(
        self, length: int = None, symbols: list = [], frame: int = None, index=None, grouped_by_symbol: bool = False
    ):
        target_symbols, missing_files = self._get_target_symbols(symbols)
        if len(missing_files) > 0:
            try:
                missing_DFS, __symbols = self._read_csv(
                    list(set(missing_files)), [], self._args["columns"], self._args["date_column"]
                )
                self.symbols.extend(list(__symbols))
            except Exception as e:
                self.logger.error(f"Filed to read csv on get_ohlc_data of CSV client by {e}")
            if missing_DFS is not None and len(missing_DFS) > 0:
                times_df = pd.DataFrame.from_dict(self.TIMES)
                min_datetime = times_df.T[0].min()
                DFS = {}
                for symbol, df in missing_DFS.items():
                    df = df[df.index >= min_datetime]
                    DFS[symbol] = df
                self.data.update(DFS)
        rates = self.__get_rates_by_chunk(length, target_symbols, frame)
        if grouped_by_symbol is False:
            rates.columns = rates.columns.swaplevel(0, 1)
            rates.sort_index(level=0, axis=1, inplace=True)
        return rates

    def get_future_rates(self, length=1, back_length=0, symbols: list = []):
        pass

    def get_current_ask(self, symbols=None):
        pass

    def get_current_bid(self, symbols=None):
        pass

    def reset(self, mode: str = None, retry=0) -> bool:
        pass

    def get_next_tick(self):
        rates = pd.concat(self.data.values(), axis=1, keys=self.data.keys())
        if self._step_index < len(rates) - 2:
            self._step_index += 1
            tick = rates.iloc[self._step_index]
            # raise index change event
            return tick, False
        else:
            if self._auto_reset:
                self._step_index = random.randint(0, len(rates))
                # raise index change event
                tick = rates.iloc[self._step_index]
                return tick, True
            else:
                self.logger.warning(f"not more data on index {self._step_index}")
            return pd.DataFrame(), True

    @property
    def max(self):
        pass

    @property
    def min(self):
        pass

    def get_additional_params(self):
        return {"chunksize": self.chunksize, "files": self.files}

    def __del__(self):
        if len(self.TRS) > 0:
            for key, value in self.TRS.items():
                value.close()

    def __len__(self):
        if self.data is not None and len(self.data) > 0:
            rates = pd.concat(self.data.values(), axis=1, keys=self.data.keys())
            return len(rates)
        return 0
